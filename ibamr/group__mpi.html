<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.17"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>IBAMR: Using IBAMR with MPI</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js", "TeX/AMSmath.js", "TeX/AMSsymbols.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script>
<script type="text/javascript" async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">IBAMR
   &#160;<span id="projectnumber">IBAMR version 0.19.</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.17 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Using IBAMR with MPI</div>  </div>
</div><!--header-->
<div class="contents">

<p>How to use <a class="el" href="namespaceIBAMR.html">IBAMR</a> with MPI and job systems.  
<a href="#details">More...</a></p>
<h1><a class="anchor" id="autotoc_md31"></a>
Tips and Tricks for using MPI with IBAMR</h1>
<h2><a class="anchor" id="autotoc_md32"></a>
Overview</h2>
<p><a class="el" href="namespaceIBAMR.html">IBAMR</a> is parallelized with MPI. MPI, the message-passing interface, is the most popular network communication standard for large-scale (dozens, hundreds, or more processors) parallelization of scientific programs at the current time. Since MPI is an industry standard there are many high-quality implementations. The two best-known implementations are OpenMPI and MPICH.</p>
<h2><a class="anchor" id="autotoc_md33"></a>
Getting Started</h2>
<h3><a class="anchor" id="autotoc_md34"></a>
Setting up MPI</h3>
<p>Whenever possible, use the copy of MPI provided by the system administrators. This is usually available through the module system.</p>
<p>MPI is a low-level library in the sense that its performance is heavily dependent on specialized hardware. In particular, to utilize common hardware on clusters (such as infiniband), the MPI library must be configured in a machine-specific way that is not trivial to do. Compiling and using your own MPI library instead of the one provided by the administrators can result in large performance problems (e.g., code can run ten times slower).</p>
<p>That being said, it may occasionally be necessary to compile MPI on your own for some reason or another. Since <a class="el" href="namespaceIBAMR.html">IBAMR</a> depends on PETSc, we recommend using PETSc to build MPI with the <code>--download-mpich</code> option.</p>
<p><a class="el" href="namespaceIBAMR.html">IBAMR</a> uses MPI to create multiple copies of the same program (the 'same instruction, multiple data' paradigm). Each copy runs in a unique process - the only way for processes to communicate is over the MPI network.</p>
<p><a class="el" href="namespaceIBAMR.html">IBAMR</a> does <em>not</em> parallelize over multiple threads. Additionally, <a class="el" href="namespaceIBAMR.html">IBAMR</a> applications do not benefit much from hyperthreading (i.e., running multiple processes on a single core). For the remainder of this document we pretend hyperthreading does not exist since it is not useful to our applications.</p>
<h3><a class="anchor" id="autotoc_md35"></a>
Running programs with MPI</h3>
<p><a class="el" href="namespaceIBAMR.html">IBAMR</a> applications are always run with a fixed number of processes. They are started by running, e.g., </p><div class="fragment"><div class="line">mpiexec -n 16 ./main3d ./input3d</div>
</div><!-- fragment --><p>to run <code>main3d</code> with input file <code>input3d</code> with <code>16</code> processors.</p>
<h3><a class="anchor" id="autotoc_md36"></a>
Determining the number of processors to use</h3>
<p>The performance of many parts of <a class="el" href="namespaceIBAMR.html">IBAMR</a> (such as the fluid solve) is limited by the rate at which data can be moved from main memory (RAM) into the CPU. The speed of this transfer can be measured with the <code>streams</code> benchmark. One can run the <code>streams</code> benchmark with PETSc by running </p><div class="fragment"><div class="line">make streams</div>
</div><!-- fragment --><p>in PETSc's root directory. The output looks something like this: </p><div class="fragment"><div class="line">6  45603.4653   Rate (MB/s) 3.81156</div>
<div class="line">7  42492.4568   Rate (MB/s) 3.55154</div>
<div class="line">8  48441.6695   Rate (MB/s) 4.04878</div>
</div><!-- fragment --><p>indicating that we can stream 48.4 GB/s with 8 processors and get a factor of 4 speedup (versus using 1 processor), measured by the time it takes to stream data. This is typical: performance usually reaches a maximum when only a fraction of a machine's processors are used.</p>
<h2><a class="anchor" id="autotoc_md37"></a>
Improving performance</h2>
<h3><a class="anchor" id="autotoc_md38"></a>
Mapping MPI processes to particular physical resources</h3>
<p>Performance is dependent on the way processors are mapped both to <em>cores</em> (the actual physical CPU) and <em>sockets</em>. Modern machines typically have multiple cores. Larger machines, such as workstations and servers, have multiple sockets, which correspond to groups of cores. My workstation has 28 physical processors which are divided among two sockets. To get this information on a Linux machine try looking at the special file <code>/proc/cpuinfo</code> (here <code>physical id</code> refers to the socket number).</p>
<p>MPI refers to the process of assigning processes to physical resources as mapping*. Mapping is usually done at the core, socket, and node level.</p>
<p>For example, if we run </p><div class="fragment"><div class="line">$ mpiexec -np 8 --map-by core ./streams</div>
<div class="line">8  24723.6086   Rate (MB/s) 2.06641</div>
</div><!-- fragment --><p>i.e., we instruct the machine to map to the first 8 cores instead of mapping evenly across the two sockets in this machine (the default). This drops to roughly half of the peak speed because we have maximized bandwidth on a single socket instead of maximizing bandwidth across two sockets.</p>
<p>Note that we get essentially identical results if we specify a number of processes equal to the number of physical processors on the machine: </p><div class="fragment"><div class="line">$ mpiexec -np 28 --map-by core ./streams</div>
<div class="line">28  48480.7239   Rate (MB/s) 4.05205</div>
<div class="line">$ mpiexec -np 28 --map-by socket ./streams</div>
<div class="line">28  48411.4946   Rate (MB/s) 4.04626</div>
</div><!-- fragment --><p>The exact binding behavior depends on the version of MPI being used. Here, I have <code>openmpi</code> on my workstation, which defaults to binding processes to sockets - that is why I get good performance with the default.</p>
<p>One can avoid doing some arithmetic by immediately assigning processes to sockets: </p><div class="fragment"><div class="line">$ mpiexec -map-by ppr:4:socket ./streams</div>
<div class="line">8  48606.2252   Rate (MB/s) 4.06254</div>
</div><!-- fragment --><p>More generally, the arguments <code>-map-by ppr:N:&lt;object&gt;</code> creates <code>N</code> times the number of <code>&lt;object&gt;</code>s copies of the application on each node.</p>
<h3><a class="anchor" id="autotoc_md39"></a>
Binding processes to particular physical resources</h3>
<p>In MPI terminology, <em>mapping</em> refers to the initial allocation of resources (starting things in certain places) while <em>binding</em> refers to adding constraints that require processes to run in specific places.</p>
<h3><a class="anchor" id="autotoc_md40"></a>
Displaying some additional information about MPI</h3>
<p>Try using the <code>-display-map</code> option to see where different processes ended up: </p><div class="fragment"><div class="line">$ mpiexec --display-map --map-by ppr:4:socket ./streams</div>
<div class="line"> Data <span class="keywordflow">for</span> JOB [22109,1] offset 0</div>
<div class="line"> </div>
<div class="line"> ========================   JOB MAP   ========================</div>
<div class="line"> </div>
<div class="line"> Data <span class="keywordflow">for</span> node: auricle Num slots: 28   Max slots: 0    Num procs: 8</div>
<div class="line">        Process OMPI jobid: [22109,1] App: 0 Process rank: 0</div>
<div class="line">        Process OMPI jobid: [22109,1] App: 0 Process rank: 1</div>
<div class="line">        Process OMPI jobid: [22109,1] App: 0 Process rank: 2</div>
<div class="line">        Process OMPI jobid: [22109,1] App: 0 Process rank: 3</div>
<div class="line">        Process OMPI jobid: [22109,1] App: 0 Process rank: 4</div>
<div class="line">        Process OMPI jobid: [22109,1] App: 0 Process rank: 5</div>
<div class="line">        Process OMPI jobid: [22109,1] App: 0 Process rank: 6</div>
<div class="line">        Process OMPI jobid: [22109,1] App: 0 Process rank: 7</div>
<div class="line"> </div>
<div class="line"> =============================================================</div>
<div class="line">8  48480.5008   Rate (MB/s) 4.05203</div>
</div><!-- fragment --><p>Similarly, <code>-report-bindings</code> creates a nice plot of where processes end up: </p><div class="fragment"><div class="line">[drwells@auricle streams]$ mpiexec -report-bindings --bind-to core --map-by ppr:4:socket ./streams</div>
<div class="line">[auricle:14958] MCW rank 4 bound to socket 1[core 14[hwt 0-1]]: [../../../../../../../../../../../../../..][BB/../../../../../../../../../../../../..]</div>
<div class="line">[auricle:14958] MCW rank 5 bound to socket 1[core 15[hwt 0-1]]: [../../../../../../../../../../../../../..][../BB/../../../../../../../../../../../..]</div>
<div class="line">[auricle:14958] MCW rank 6 bound to socket 1[core 16[hwt 0-1]]: [../../../../../../../../../../../../../..][../../BB/../../../../../../../../../../..]</div>
<div class="line">[auricle:14958] MCW rank 7 bound to socket 1[core 17[hwt 0-1]]: [../../../../../../../../../../../../../..][../../../BB/../../../../../../../../../..]</div>
<div class="line">[auricle:14958] MCW rank 0 bound to socket 0[core 0[hwt 0-1]]:  [BB/../../../../../../../../../../../../..][../../../../../../../../../../../../../..]</div>
<div class="line">[auricle:14958] MCW rank 1 bound to socket 0[core 1[hwt 0-1]]:  [../BB/../../../../../../../../../../../..][../../../../../../../../../../../../../..]</div>
<div class="line">[auricle:14958] MCW rank 2 bound to socket 0[core 2[hwt 0-1]]:  [../../BB/../../../../../../../../../../..][../../../../../../../../../../../../../..]</div>
<div class="line">[auricle:14958] MCW rank 3 bound to socket 0[core 3[hwt 0-1]]:  [../../../BB/../../../../../../../../../..][../../../../../../../../../../../../../..]</div>
<div class="line">8  48484.6851   Rate (MB/s) 4.05238</div>
</div><!-- fragment --><p>Note that this defaults to using all known sockets automatically, but if we supply <code>-n 4</code> as well then we obtain </p><div class="fragment"><div class="line">$ mpiexec -n 4 --bind-to core -report-bindings --map-by ppr:4:socket ./streams</div>
<div class="line">[auricle:15285] MCW rank 2 bound to socket 0[core 2[hwt 0-1]]: [../../BB/../../../../../../../../../../..][../../../../../../../../../../../../../..]</div>
<div class="line">[auricle:15285] MCW rank 3 bound to socket 0[core 3[hwt 0-1]]: [../../../BB/../../../../../../../../../..][../../../../../../../../../../../../../..]</div>
<div class="line">[auricle:15285] MCW rank 0 bound to socket 0[core 0[hwt 0-1]]: [BB/../../../../../../../../../../../../..][../../../../../../../../../../../../../..]</div>
<div class="line">[auricle:15285] MCW rank 1 bound to socket 0[core 1[hwt 0-1]]: [../BB/../../../../../../../../../../../..][../../../../../../../../../../../../../..]</div>
<div class="line">4  24219.5586   Rate (MB/s) 2.02429</div>
</div><!-- fragment --><p>i.e., we get a single socket. If we ask it to do something impossible we get a useful error message: </p><div class="fragment"><div class="line">$ mpiexec -n 8 --bind-to core -report-bindings --map-by ppr:2:socket ./streams</div>
<div class="line">--------------------------------------------------------------------------</div>
<div class="line">Your job has requested more processes than the ppr <span class="keywordflow">for</span></div>
<div class="line"><span class="keyword">this</span> topology can support:</div>
<div class="line"> </div>
<div class="line">  App: ./streams</div>
<div class="line">  Number of procs:  8</div>
<div class="line">  PPR: 2:socket</div>
<div class="line"> </div>
<div class="line">Please revise the conflict and <span class="keywordflow">try</span> again.</div>
<div class="line">--------------------------------------------------------------------------</div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md41"></a>
Setting up scripts for SLURM</h3>
<p>This section assumes that you are using SLURM to manage running <a class="el" href="namespaceIBAMR.html">IBAMR</a> on a cluster.</p>
<p>Scientific programs, like <a class="el" href="namespaceIBAMR.html">IBAMR</a> applications, typically run for long periods of time and require known amount of resources: for example, one might want to run their <a class="el" href="namespaceIBAMR.html">IBAMR</a> application with 100 processors for four days.</p>
<p>To allocate resources fairly most clusters use a scheduling program to assign jobs to actual computational hardware.</p>
<p>Resource allocation typically require submitting a shell script describing both what is required (e.g., amount of time, number of nodes, etc.) and also a description of the work. Here is an example job script for running streams: </p><div class="fragment"><div class="line"><span class="preprocessor">#!/bin/bash</span></div>
<div class="line"><span class="preprocessor">#SBATCH --partition=debug_queue</span></div>
<div class="line"><span class="preprocessor">#SBATCH --nodes=2</span></div>
<div class="line"><span class="preprocessor">#SBATCH --time=00:10:00</span></div>
<div class="line"><span class="preprocessor">#SBATCH --job-name=run-streams</span></div>
<div class="line"><span class="preprocessor"># When to send an email - here, send an email when the job starts, finishes,</span></div>
<div class="line"><span class="preprocessor"># or encounters an error.</span></div>
<div class="line"><span class="preprocessor">#SBATCH --mail-type=begin,end,error</span></div>
<div class="line"><span class="preprocessor"># Email address to use.</span></div>
<div class="line"><span class="preprocessor">#SBATCH --email-user=user@email.com</span></div>
<div class="line"><span class="preprocessor"># not necessary for most queues on dogwood (those default to exclusive node</span></div>
<div class="line"><span class="preprocessor"># access), but it can&#39;t hurt to ask. This guarantees that we are the only</span></div>
<div class="line"><span class="preprocessor"># one running things on the provided nodes. Since we typically do not use</span></div>
<div class="line"><span class="preprocessor"># every processor (but we do want access to all the memory bandwidth) its</span></div>
<div class="line"><span class="preprocessor"># important to provide this on machines where it is not the default.</span></div>
<div class="line"><span class="preprocessor">#SBATCH --exclusive</span></div>
<div class="line"> </div>
<div class="line">module load openmpi_4.0.1/gcc_9.1.0</div>
<div class="line"> </div>
<div class="line">cd ~/Documents/Code/C/streams</div>
<div class="line">mpiexec -map-by ppr:10:socket -bind-to core ./streams</div>
</div><!-- fragment --><p>Here <code>mpiexec</code> knows to work with SLURM to create a total of 40 processes - i.e., it will automatically detect the number of nodes and start twenty processes on each node (and ten on each socket).</p>
<p>You can submit your job to the job scheduler by running <code>sbatch script.sb</code>, where <code>script.sb</code> is the file name of the script you want to run (such as the script given above).</p>
<h3><a class="anchor" id="autotoc_md42"></a>
SLURM tips and tricks</h3>
<h4><a class="anchor" id="autotoc_md43"></a>
keeping track of jobs</h4>
<p>Try running <code>squeue -u username</code> (where <code>username</code> is your username) to see the current status of your jobs. Running </p><div class="fragment"><div class="line">watch -n 1 squeue -u username</div>
</div><!-- fragment --><p>will update the result every second.</p>
<h4><a class="anchor" id="autotoc_md44"></a>
viewing all jobs</h4>
<p>Try running <code>squeue</code> to print <em>all</em> current jobs on the cluster. Try running <code>finger username</code> to get more information on a certain user.</p>
<h3><a class="anchor" id="autotoc_md45"></a>
Final advice</h3>
<p>From the PETSc manual:</p>
<p>"For a typical, memory bandwidth-limited PETSc application, the primary consideration in placing MPI processes is ensuring that processes are evenly distributed among sockets, and hence using all available memory channels."</p>
<p>The same holds for <a class="el" href="namespaceIBAMR.html">IBAMR</a>: one should use the streams benchmark to determine how many processes should be put on a socket and then use <code>--map-by ppr:N:socket</code> to set up a run with <code>N</code> processes per socket. <code>--bind-to core</code> is a good default but might not be the optimal choice on all hardware. </p>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.17
</small></address>
</body>
</html>
